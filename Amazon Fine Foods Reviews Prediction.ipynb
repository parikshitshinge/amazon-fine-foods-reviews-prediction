{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This dataset consists of reviews of fine foods from amazon. <br>The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. <br> It also includes reviews from all other Amazon categories. <br><br>Data set: https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "#### Dataset info:\n",
    "\n",
    "Number of reviews: 568,454 <br>\n",
    "Number of users: 256,059<br>\n",
    "Number of products: 74,258<br>\n",
    "Timespan: Oct 1999 - Oct 2012<br>\n",
    "Number of Attributes/Columns in data: 10<br><br>\n",
    "\n",
    "<b>Attribute Information:</b><br>\n",
    "\n",
    "Id - unique identifier for the review<br>\n",
    "ProductId - unique identifier for the product<br>\n",
    "UserId - unqiue identifier for the user<br>\n",
    "ProfileName<br>\n",
    "HelpfulnessNumerator - number of users who found the review helpful<br>\n",
    "HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not<br>\n",
    "Score - rating between 1 and 5<br>\n",
    "Time - timestamp for the review<br>\n",
    "Summary - brief summary of the review<br>\n",
    "Text - text of the review<br>\n",
    "\n",
    "<br>\n",
    "The dataset is .sqlite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "\n",
    "# normal DS imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3 \n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# NLP imports\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset using sqlite\n",
    "\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "\n",
    "original_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews \"\"\", con)\n",
    "\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "#### Our objective is to determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2) given a review\n",
    "\n",
    "Since we want to predict the reviews which are either positive or negative, we can drop the data where 'Score' is 3. <br>\n",
    "We will also change 'Score' column logic to indicate either positive or negative and predict this ourselves based on sentiment in 'Text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = original_data[original_data['Score']!=3]\n",
    "filtered_data.drop('Score', axis=1)\n",
    "\n",
    "\n",
    "# Lets replace 'Score' as follows \n",
    "def partition(x):\n",
    "    if x > 3:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "actualScores = filtered_data['Score']\n",
    "positiveNegative = actualScores.map(partition)\n",
    "filtered_data['Score'] = positiveNegative\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "cleaning.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation 1:</b> We observed that when a product has variations, amazon combines the reviews for all the variations. Since it make sense to drop the variation product reviews and keep distinct ones. The reason being, it will be very easy for our ML model to predict the review of product variations based on original product review. <b> Lets remove the duplicates </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets sort data based on ProductId in ascending order\n",
    "sorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets do deduplciation\n",
    "final = sorted_data.drop_duplicates(subset={\"UserId\", \"ProfileName\",\"Time\", \"Text\"}, keep=\"first\", inplace=False)\n",
    "\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how much data is preserved after deduplication\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation 2: </b>We know that 'HelpfulnessNumerator' should be less than 'HelpfulnessDenominator'.<br>Few rows violet this. <b>Lets drop 'em.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64421</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44736</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId              ProfileName  \\\n",
       "64421  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "44736  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "64421                     3                       1      1  1224892800   \n",
       "44736                     3                       2      1  1212883200   \n",
       "\n",
       "                                            Summary  \\\n",
       "64421             Bought This for My Son at College   \n",
       "44736  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                    Text  \n",
       "64421  My son loves spaghetti so I didn't hesitate or...  \n",
       "44736  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[final['HelpfulnessNumerator'] > final['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[final['HelpfulnessNumerator'] <= final['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    307061\n",
       "0     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>Observation 3: </b>We observed that the food reviews data has some reviews of books. We can use 'Text' & 'Summary' columns to identify whether they are book reviews if words like 'book', 'read', 'reading' are there.<br>\n",
    " There is also a possibility that some food reviews contain words like these. It's upto us whether to delete this data or not. Since we got this data from data team, we will consider that these reviews are for food & not for book.<br>\n",
    " <b>If there will be need to delete this data, just make below cell as 'Code' and run.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def apply_mask_summary(data,regex_string):\n",
    "    mask = data.Summary.str.lower().str.contains(regex_string)\n",
    "    data.drop(data[mask].index, inplace=True)\n",
    "\n",
    "def apply_mask_text(data,regex_string):\n",
    "    mask = data.Text.str.lower().str.contains(regex_string)\n",
    "    data.drop(data[mask].index, inplace=True)\n",
    "\n",
    "\n",
    "apply_mask_summary(final,re.compile(r\"\\bbook\\b\"))\n",
    "apply_mask_summary(final,re.compile(r\"\\bread\\b\"))\n",
    "\n",
    "apply_mask_text(final,re.compile(r\"\\bbook\\b\"))\n",
    "apply_mask_text(final,re.compile(r\"\\bread\\b\"))\n",
    "\n",
    "apply_mask_summary(final,re.compile(r\"\\bbooks\\b\"))\n",
    "apply_mask_summary(final,re.compile(r\"\\breads\\b\"))\n",
    "\n",
    "apply_mask_text(final,re.compile(r\"\\bbooks\\b\"))\n",
    "apply_mask_text(final,re.compile(r\"\\breads\\b\"))\n",
    "\n",
    "apply_mask_summary(final,re.compile(r\"\\breading\\b\"))\n",
    "apply_mask_text(final,re.compile(r\"\\breading\\b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing\n",
    "\n",
    "1. Remove the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation 1: </b>We also observed that few of the text reviews include hyperlink/URLs. <b>Lets remove those</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation 2: </b>We also observed that few of the text reviews include HTML tags. <b>Lets remove those</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation 3: </b>Lets remove punctuations<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanpunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation 4: </b>We observed that the text reviews include contractions like he's, they're, you've. <br>We need to expand these contractions so that 'he is' will be == he's<br>\n",
    "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation 5: </b>Remove stop words & do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "sno = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do all above operations now\n",
    "\n",
    "i = 0\n",
    "str1 = ''\n",
    "final_string = []\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "s = ''\n",
    "\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence = []\n",
    "    sent = cleanhtml(sent) # Clean HTML \n",
    "    \n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if( (cleaned_words.isalpha()) & (len(cleaned_words)>2) ):\n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s = (sno.stem(cleaned_words.lower())).encode('utf-8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if(final['Score'].values)[i] == 1:\n",
    "                        all_positive_words.append(s)\n",
    "                    if(final['Score'].values)[i] == 0:\n",
    "                        all_negative_words.append(s)\n",
    "                        \n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    str1 = b\" \".join(filtered_sentence) # Final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText'] = final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150523</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>b'witti littl book make son laugh loud recit c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150505</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "      <td>b'grew read sendak book watch realli rosi movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150506</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "      <td>b'fun way children learn month year learn poem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150507</th>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "      <td>b'great littl book read nice rhythm well good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150508</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "      <td>b'book poetri month year goe month cute littl ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "150523  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "150505  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "150506  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "150507  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "150508  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "150523                     0                       0      1   939340800   \n",
       "150505                     1                       1      1  1194739200   \n",
       "150506                     1                       1      1  1191456000   \n",
       "150507                     1                       1      1  1076025600   \n",
       "150508                     3                       4      1  1018396800   \n",
       "\n",
       "                                           Summary  \\\n",
       "150523                   EVERY book is educational   \n",
       "150505  Love the book, miss the hard cover version   \n",
       "150506               chicken soup with rice months   \n",
       "150507      a good swingy rhythm for reading aloud   \n",
       "150508             A great way to learn the months   \n",
       "\n",
       "                                                     Text  \\\n",
       "150523  this witty little book makes my son laugh at l...   \n",
       "150505  I grew up reading these Sendak books, and watc...   \n",
       "150506  This is a fun way for children to learn their ...   \n",
       "150507  This is a great little book to read aloud- it ...   \n",
       "150508  This is a book of poetry about the months of t...   \n",
       "\n",
       "                                              CleanedText  \n",
       "150523  b'witti littl book make son laugh loud recit c...  \n",
       "150505  b'grew read sendak book watch realli rosi movi...  \n",
       "150506  b'fun way children learn month year learn poem...  \n",
       "150507  b'great littl book read nice rhythm well good ...  \n",
       "150508  b'book poetri month year goe month cute littl ...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store this final table in sqlite\n",
    "\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "\n",
    "final.to_sql('Reviews', conn, schema=None, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick analysis to decide n-gram range\n",
    "\n",
    "Now we have list of words describing positive & negative reviews lets analyse them.\n",
    "<br>Lets begin by getting the frequency distribution of the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_positive = FreqDist(all_positive_words)\n",
    "freq_dist_negative = FreqDist(all_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791), (b'tea', 83888), (b'coffe', 78814), (b'make', 75107), (b'get', 72125), (b'food', 64802), (b'would', 55568), (b'time', 55264), (b'buy', 54198), (b'realli', 52715), (b'eat', 52004)]\n"
     ]
    }
   ],
   "source": [
    "print(freq_dist_positive.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12754), (b'dont', 11877), (b'tea', 11665), (b'even', 11085), (b'box', 10844), (b'amazon', 10073), (b'make', 9840)]\n"
     ]
    }
   ],
   "source": [
    "print(freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observe, 'like' is also categorized into negative which means it must be 'not like'. This indicates that we can use bi-gram to make model stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 71624)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BoW for uni-gram\n",
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['CleanedText'].values)\n",
    "\n",
    "final_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2923725)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BoW for bi-gram\n",
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "final_counts = count_vect.fit_transform(final['CleanedText'].values)\n",
    "\n",
    "final_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above can see massive increase in dimensions if we apply bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
